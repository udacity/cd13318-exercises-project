{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Demo 4: Chain-of-Thought Prompting\n",
        "\n",
        "In this demo, you'll learn how to make LLMs think step-by-step for better reasoning! We'll explore:\n",
        "\n",
        "1. **Basic vs CoT prompting** - See the difference\n",
        "2. **Mathematical reasoning** - Solve problems step-by-step\n",
        "3. **Business logic** - Customer service decision making\n",
        "4. **Few-shot CoT** - Teaching by example\n",
        "5. **When to use CoT** - Cost vs benefit analysis\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "By the end of this demo, you'll understand:\n",
        "- How chain-of-thought prompting improves accuracy\n",
        "- When to use CoT vs direct prompting\n",
        "- How to structure prompts for step-by-step reasoning\n",
        "- The tradeoffs between accuracy and cost"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n",
        "\n",
        "First, let's import the required libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "import os\n",
        "import json\n",
        "\n",
        "# Initialize OpenAI client\n",
        "api_key = os.getenv(\"OPENAI_API_KEY\", \"your-api-key-here\")\n",
        "client = OpenAI(\n",
        "    api_key=api_key,\n",
        "    base_url=\"https://openai.vocareum.com/v1\" if api_key.startswith(\"voc\") else None\n",
        ")\n",
        "\n",
        "print(\"âœ… OpenAI client initialized!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 1: The Problem - Basic Prompting Limitations\n",
        "\n",
        "Let's start by showing where basic prompting can fail."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def ask_llm(prompt, system_prompt=\"You are a helpful assistant.\", temperature=0):\n",
        "    \"\"\"Simple helper to query the LLM.\"\"\"\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        temperature=temperature,\n",
        "        max_tokens=500\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "# A tricky math problem\n",
        "math_problem = \"\"\"If a product costs $50 and is on sale for 30% off, \n",
        "and then I apply an additional 10% off coupon to the sale price, \n",
        "what is the final price?\"\"\"\n",
        "\n",
        "print(\"ðŸ¤” Basic Prompting Approach:\\n\")\n",
        "print(f\"Question: {math_problem}\\n\")\n",
        "\n",
        "basic_answer = ask_llm(math_problem)\n",
        "print(f\"Answer: {basic_answer}\")\n",
        "print(\"\\n\" + \"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ðŸ’¡ What Happened?\n",
        "\n",
        "The LLM might:\n",
        "- Jump straight to an answer without showing work\n",
        "- Make calculation errors\n",
        "- Apply discounts in the wrong order\n",
        "\n",
        "Let's see if we can do better with chain-of-thought!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 2: Chain-of-Thought Prompting - The Solution\n",
        "\n",
        "Now let's ask the LLM to **think step-by-step**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Same problem, but with CoT instruction\n",
        "cot_prompt = f\"\"\"{math_problem}\n",
        "\n",
        "Let's solve this step-by-step:\n",
        "\"\"\"\n",
        "\n",
        "print(\"ðŸ§  Chain-of-Thought Approach:\\n\")\n",
        "print(f\"Question: {math_problem}\\n\")\n",
        "print(\"Instruction: Let's solve this step-by-step:\\n\")\n",
        "\n",
        "cot_answer = ask_llm(cot_prompt)\n",
        "print(f\"Answer:\\n{cot_answer}\")\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "\n",
        "print(\"\\nâœ… Notice how CoT prompting:\")\n",
        "print(\"  1. Shows intermediate steps\")\n",
        "print(\"  2. Makes the reasoning transparent\")\n",
        "print(\"  3. Reduces calculation errors\")\n",
        "print(\"  4. Is easier to verify and debug\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 3: More Complex Reasoning - Customer Service Scenarios\n",
        "\n",
        "Let's apply CoT to a real business problem!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Customer service scenario\n",
        "scenario = \"\"\"A customer ordered 3 items:\n",
        "- Item A: $30 (arrived on time, no issues)\n",
        "- Item B: $45 (arrived damaged)\n",
        "- Item C: $60 (never arrived)\n",
        "\n",
        "The customer wants a full refund of $135 for the entire order.\n",
        "\n",
        "Our policy:\n",
        "- Damaged items: Full refund OR replacement\n",
        "- Lost items: Full refund after 14 days from order date\n",
        "- Working items: No refund (unless quality issue)\n",
        "- Shipping: Refunded only if entire order is refunded\n",
        "\n",
        "Today is 10 days after the order date.\n",
        "\n",
        "Should we approve the full refund request?\"\"\"\n",
        "\n",
        "# Without CoT\n",
        "print(\"âŒ Without Chain-of-Thought:\\n\")\n",
        "basic_decision = ask_llm(scenario)\n",
        "print(basic_decision)\n",
        "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
        "\n",
        "# With CoT\n",
        "cot_scenario = f\"\"\"{scenario}\n",
        "\n",
        "Let's analyze this step-by-step:\n",
        "1. Review each item's situation\n",
        "2. Apply relevant policies\n",
        "3. Calculate what should be refunded\n",
        "4. Make a final decision\n",
        "\"\"\"\n",
        "\n",
        "print(\"âœ… With Chain-of-Thought:\\n\")\n",
        "cot_decision = ask_llm(cot_scenario)\n",
        "print(cot_decision)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ðŸ’¡ Key Insight\n",
        "\n",
        "Chain-of-thought prompting helps with:\n",
        "- **Multi-step logic**: Multiple rules need to be applied\n",
        "- **Policy compliance**: Ensure each rule is considered\n",
        "- **Transparency**: See exactly why a decision was made\n",
        "- **Debugging**: Identify where reasoning went wrong"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 4: Few-Shot Chain-of-Thought\n",
        "\n",
        "You can **teach** the LLM how to reason by showing examples!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Few-shot CoT prompt with examples\n",
        "few_shot_prompt = \"\"\"I'll help you solve pricing problems. Here are some examples:\n",
        "\n",
        "Example 1:\n",
        "Question: A $100 item is 20% off. What's the final price?\n",
        "Reasoning:\n",
        "Step 1: Calculate discount amount: $100 Ã— 0.20 = $20\n",
        "Step 2: Subtract from original: $100 - $20 = $80\n",
        "Answer: $80\n",
        "\n",
        "Example 2:\n",
        "Question: A $80 item has a 25% discount, then an additional $10 off. What's the final price?\n",
        "Reasoning:\n",
        "Step 1: Apply percentage discount: $80 Ã— 0.25 = $20 off\n",
        "Step 2: Price after first discount: $80 - $20 = $60\n",
        "Step 3: Apply additional discount: $60 - $10 = $50\n",
        "Answer: $50\n",
        "\n",
        "Now solve this:\n",
        "Question: A $120 item is 15% off. Then I have a coupon for an additional 20% off the discounted price. \n",
        "Plus, there's a $5 loyalty discount. What's the final price?\n",
        "\n",
        "Reasoning:\n",
        "\"\"\"\n",
        "\n",
        "print(\"ðŸŽ“ Few-Shot Chain-of-Thought Learning:\\n\")\n",
        "answer = ask_llm(few_shot_prompt)\n",
        "print(answer)\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "\n",
        "print(\"\\nâœ… The model learned the reasoning pattern from examples!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 5: Comparing Different Prompting Strategies\n",
        "\n",
        "Let's systematically compare approaches on the same problem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test problem\n",
        "test_problem = \"\"\"A customer service agent needs to decide whether to escalate a case.\n",
        "\n",
        "Customer issue: Ordered a laptop 3 weeks ago, never received it. Tracking shows \n",
        "it was delivered to the wrong address. Customer is angry and threatening to leave \n",
        "a negative review. Customer has been with us for 5 years and spent over $10,000.\n",
        "\n",
        "Escalation criteria:\n",
        "- Financial loss > $500: Escalate\n",
        "- Customer lifetime value > $5,000 AND angry: Escalate\n",
        "- Legal threat: Escalate\n",
        "- Simple request < $100: Don't escalate\n",
        "\n",
        "Should this be escalated?\"\"\"\n",
        "\n",
        "strategies = {\n",
        "    \"Direct Answer\": test_problem,\n",
        "    \n",
        "    \"Zero-shot CoT\": f\"{test_problem}\\n\\nLet's think step-by-step:\",\n",
        "    \n",
        "    \"Structured CoT\": f\"\"\"{test_problem}\n",
        "\n",
        "Let's analyze this systematically:\n",
        "1. Identify the key facts\n",
        "2. Check each escalation criterion\n",
        "3. Make a decision with justification\n",
        "\"\"\",\n",
        "    \n",
        "    \"Explicit Reasoning\": f\"\"\"{test_problem}\n",
        "\n",
        "Please provide:\n",
        "1. List of relevant facts from the case\n",
        "2. Each escalation criterion and whether it applies (Yes/No with reason)\n",
        "3. Final decision\n",
        "4. Recommended next actions\n",
        "\"\"\"\n",
        "}\n",
        "\n",
        "print(\"ðŸ“Š Comparing Prompting Strategies:\\n\")\n",
        "print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "for strategy_name, prompt in strategies.items():\n",
        "    print(f\"ðŸ”¹ {strategy_name}:\\n\")\n",
        "    answer = ask_llm(prompt)\n",
        "    print(answer)\n",
        "    print(\"\\n\" + \"=\"*80 + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 6: When to Use Chain-of-Thought\n",
        "\n",
        "CoT isn't always necessary. Let's analyze the tradeoffs!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tiktoken\n",
        "\n",
        "# Initialize tokenizer\n",
        "encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
        "\n",
        "# Test cases\n",
        "test_cases = [\n",
        "    {\n",
        "        \"name\": \"Simple lookup\",\n",
        "        \"question\": \"What's our return policy?\",\n",
        "        \"complexity\": \"Low\",\n",
        "        \"use_cot\": False,\n",
        "        \"reason\": \"Direct fact retrieval, no reasoning needed\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Multi-step calculation\",\n",
        "        \"question\": \"Calculate final price with multiple discounts\",\n",
        "        \"complexity\": \"Medium\",\n",
        "        \"use_cot\": True,\n",
        "        \"reason\": \"Math with multiple steps, errors common without CoT\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Yes/No question\",\n",
        "        \"question\": \"Is the store open on Sundays?\",\n",
        "        \"complexity\": \"Low\",\n",
        "        \"use_cot\": False,\n",
        "        \"reason\": \"Binary answer, no reasoning required\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Policy application\",\n",
        "        \"question\": \"Should we refund this complex order with multiple issues?\",\n",
        "        \"complexity\": \"High\",\n",
        "        \"use_cot\": True,\n",
        "        \"reason\": \"Multiple rules, edge cases, needs justification\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Creative writing\",\n",
        "        \"question\": \"Write a welcome email for new customers\",\n",
        "        \"complexity\": \"Low\",\n",
        "        \"use_cot\": False,\n",
        "        \"reason\": \"Creative task, not logical reasoning\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Root cause analysis\",\n",
        "        \"question\": \"Why did our sales drop last quarter?\",\n",
        "        \"complexity\": \"High\",\n",
        "        \"use_cot\": True,\n",
        "        \"reason\": \"Requires systematic analysis of multiple factors\"\n",
        "    }\n",
        "]\n",
        "\n",
        "print(\"ðŸŽ¯ When to Use Chain-of-Thought: Decision Guide\\n\")\n",
        "print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "for case in test_cases:\n",
        "    recommendation = \"âœ… USE CoT\" if case[\"use_cot\"] else \"âŒ DON'T use CoT\"\n",
        "    print(f\"ðŸ“‹ {case['name']}\")\n",
        "    print(f\"   Question type: {case['question']}\")\n",
        "    print(f\"   Complexity: {case['complexity']}\")\n",
        "    print(f\"   Recommendation: {recommendation}\")\n",
        "    print(f\"   Reason: {case['reason']}\")\n",
        "    print()\n",
        "\n",
        "# Cost comparison\n",
        "print(\"=\"*80 + \"\\n\")\n",
        "print(\"ðŸ’° Cost Comparison:\\n\")\n",
        "\n",
        "basic_prompt = \"What is 2+2?\"\n",
        "cot_prompt = \"What is 2+2? Let's solve this step-by-step:\"\n",
        "\n",
        "basic_tokens = len(encoding.encode(basic_prompt))\n",
        "cot_tokens = len(encoding.encode(cot_prompt))\n",
        "\n",
        "# Estimate output tokens (CoT generates more)\n",
        "basic_output = 10\n",
        "cot_output = 50\n",
        "\n",
        "print(f\"Basic prompt: {basic_tokens} input + {basic_output} output = {basic_tokens + basic_output} total tokens\")\n",
        "print(f\"CoT prompt: {cot_tokens} input + {cot_output} output = {cot_tokens + cot_output} total tokens\")\n",
        "print(f\"\\nCoT uses ~{((cot_tokens + cot_output) / (basic_tokens + basic_output)):.1f}x more tokens\")\n",
        "print(\"\\nâš ï¸ CoT is more expensive, but worth it for complex reasoning tasks!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 7: Advanced CoT Pattern - Self-Consistency\n",
        "\n",
        "Generate multiple reasoning paths and choose the most common answer!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def self_consistency_cot(question, num_samples=3):\n",
        "    \"\"\"Generate multiple CoT reasoning paths and find consensus.\"\"\"\n",
        "    prompt = f\"{question}\\n\\nLet's solve this step-by-step:\"\n",
        "    \n",
        "    answers = []\n",
        "    \n",
        "    print(f\"ðŸ”„ Generating {num_samples} independent reasoning paths...\\n\")\n",
        "    \n",
        "    for i in range(num_samples):\n",
        "        response = ask_llm(prompt, temperature=0.7)  # Add some randomness\n",
        "        answers.append(response)\n",
        "        print(f\"Path {i+1}:\")\n",
        "        print(response)\n",
        "        print(\"\\n\" + \"-\"*80 + \"\\n\")\n",
        "    \n",
        "    return answers\n",
        "\n",
        "# Test problem\n",
        "tricky_question = \"\"\"A store has a '3 for $10' deal on items that normally cost $4 each.\n",
        "If I buy 5 items, how much do I pay?\"\"\"\n",
        "\n",
        "print(\"ðŸ§ª Self-Consistency Chain-of-Thought:\\n\")\n",
        "print(f\"Question: {tricky_question}\\n\")\n",
        "print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "paths = self_consistency_cot(tricky_question, num_samples=3)\n",
        "\n",
        "print(\"\\nðŸ’¡ By comparing multiple reasoning paths, we can:\")\n",
        "print(\"  1. Identify the most reliable answer\")\n",
        "print(\"  2. Catch calculation errors\")\n",
        "print(\"  3. Understand different valid interpretations\")\n",
        "print(\"  4. Increase confidence in complex problems\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸŽ¯ Key Takeaways\n",
        "\n",
        "1. **CoT = Step-by-Step Reasoning** - Explicitly ask the LLM to show its work\n",
        "\n",
        "2. **Improves Accuracy** - Especially for math, logic, and multi-step problems\n",
        "\n",
        "3. **Increases Transparency** - You can see and verify the reasoning\n",
        "\n",
        "4. **Few-shot CoT** - Teach reasoning patterns through examples\n",
        "\n",
        "5. **Trade-off: Cost vs Accuracy** - CoT uses more tokens but gives better results\n",
        "\n",
        "6. **When to Use CoT**:\n",
        "   - âœ… Multi-step calculations\n",
        "   - âœ… Complex policy/rule application\n",
        "   - âœ… Root cause analysis\n",
        "   - âœ… When you need to verify reasoning\n",
        "   - âŒ Simple lookups\n",
        "   - âŒ Creative writing\n",
        "   - âŒ Binary yes/no questions\n",
        "\n",
        "## ðŸ’¡ CoT Prompt Patterns\n",
        "\n",
        "**Zero-shot CoT** (simplest):\n",
        "```\n",
        "[Question]\n",
        "Let's think step-by-step:\n",
        "```\n",
        "\n",
        "**Structured CoT**:\n",
        "```\n",
        "[Question]\n",
        "Let's solve this systematically:\n",
        "1. [First step description]\n",
        "2. [Second step description]\n",
        "3. [Final decision]\n",
        "```\n",
        "\n",
        "**Few-shot CoT**:\n",
        "```\n",
        "Example 1: [Question] \n",
        "Reasoning: [Step by step]\n",
        "Answer: [Result]\n",
        "\n",
        "Example 2: [Question]\n",
        "Reasoning: [Step by step]\n",
        "Answer: [Result]\n",
        "\n",
        "Now solve: [Your question]\n",
        "```\n",
        "\n",
        "## ðŸš€ Try It Yourself!\n",
        "\n",
        "Experiments to run:\n",
        "1. Compare accuracy on math problems with and without CoT\n",
        "2. Test different CoT prompt structures\n",
        "3. Build a customer service decision tree using CoT\n",
        "4. Try self-consistency on challenging problems\n",
        "5. Measure the token cost increase from CoT"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
