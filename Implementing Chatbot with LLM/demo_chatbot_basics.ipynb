{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo 1: Building a Basic Chatbot with LLMs\n",
    "\n",
    "In this demo, you'll learn how to interact with OpenAI's API to build a simple chatbot. We'll explore:\n",
    "\n",
    "1. **Basic API calls** - Making your first request\n",
    "2. **Conversation context** - Maintaining chat history\n",
    "3. **System prompts** - Controlling bot behavior\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this demo, you'll understand:\n",
    "- How to make basic API calls to LLMs\n",
    "- How to build a stateful conversation\n",
    "- How system prompts shape bot behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's import the required libraries and set up our API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Set your API key (get from environment or replace with your key)\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\", \"your-api-key-here\")\n",
    "\n",
    "# Initialize the client\n",
    "# For Vocareum keys, use: base_url=\"https://openai.vocareum.com/v1\"\n",
    "client = OpenAI(\n",
    "    api_key=api_key,\n",
    "    base_url=\"https://openai.vocareum.com/v1\" if api_key.startswith(\"voc\") else None\n",
    ")\n",
    "\n",
    "print(\"‚úÖ OpenAI client initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Basic API Call\n",
    "\n",
    "Let's start with a simple question and see how the LLM responds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple question\n",
    "question = \"What is an embedding?\"\n",
    "\n",
    "# Make the API call\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": question}\n",
    "    ],\n",
    "    temperature=0.7,\n",
    "    max_tokens=100\n",
    ")\n",
    "\n",
    "# Extract the answer\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"\\nAnswer: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ü§î What Just Happened?\n",
    "\n",
    "The LLM generated a response by **predicting the next most likely word** repeatedly. This simple mechanism enables complex, coherent responses!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Building a Stateful Conversation\n",
    "\n",
    "LLMs are **stateless** - they don't remember previous messages unless you include them! Let's build a proper conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize conversation history\n",
    "conversation = []\n",
    "\n",
    "def chat(user_message):\n",
    "    \"\"\"Send a message and get a response, maintaining conversation history.\"\"\"\n",
    "    \n",
    "    # Add user message to history\n",
    "    conversation.append({\n",
    "        \"role\": \"user\",\n",
    "        \"content\": user_message\n",
    "    })\n",
    "    \n",
    "    # Make API call with full conversation history\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=conversation,\n",
    "        temperature=0.7,\n",
    "        max_tokens=150\n",
    "    )\n",
    "    \n",
    "    # Get assistant's response\n",
    "    assistant_message = response.choices[0].message.content\n",
    "    \n",
    "    # Add assistant's response to history\n",
    "    conversation.append({\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": assistant_message\n",
    "    })\n",
    "    \n",
    "    return assistant_message\n",
    "\n",
    "# Have a conversation!\n",
    "print(\"User: What's the weather like today?\")\n",
    "response1 = chat(\"What's the weather like today?\")\n",
    "print(f\"Bot: {response1}\\n\")\n",
    "\n",
    "print(\"User: Should I bring an umbrella?\")\n",
    "response2 = chat(\"Should I bring an umbrella?\")  # References previous context!\n",
    "print(f\"Bot: {response2}\\n\")\n",
    "\n",
    "print(\"User: Thanks! What about tomorrow?\")\n",
    "response3 = chat(\"Thanks! What about tomorrow?\")  # Still maintaining context\n",
    "print(f\"Bot: {response3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìù View Conversation History\n",
    "\n",
    "Let's see what we're sending to the API with each request:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Current conversation history:\\n\")\n",
    "print(json.dumps(conversation, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: System Prompts - Controlling Behavior\n",
    "\n",
    "System prompts are **secret instructions** that define how the bot should behave. The user never sees them, but they dramatically affect responses!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bot_with_personality(system_prompt):\n",
    "    \"\"\"Create a bot with a specific personality.\"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": \"Tell me about troubleshooting.\"}\n",
    "    ]\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=messages,\n",
    "        temperature=0.7,\n",
    "        max_tokens=100\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Test different personalities\n",
    "print(\"ü§ñ Professional Bot:\")\n",
    "professional = create_bot_with_personality(\n",
    "    \"You are a professional tech support assistant. Be formal and concise.\"\n",
    ")\n",
    "print(professional)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "print(\"üòä Friendly Bot:\")\n",
    "friendly = create_bot_with_personality(\n",
    "    \"You are a friendly, enthusiastic tech support assistant. Use casual language and be upbeat!\"\n",
    ")\n",
    "print(friendly)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "print(\"üé≠ Pirate Bot (just for fun!):\")\n",
    "pirate = create_bot_with_personality(\n",
    "    \"You are a pirate tech support assistant. Respond in pirate speak with 'arr' and 'matey'!\"\n",
    ")\n",
    "print(pirate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Building a Tech Support Bot\n",
    "\n",
    "Now let's put it all together to build a simple tech support chatbot!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a tech support bot\n",
    "tech_support_bot = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"\"\"You are a helpful tech support assistant for TechCo, a software company.\n",
    "        \n",
    "Your responsibilities:\n",
    "- Troubleshoot software issues and bugs\n",
    "- Help with installation and setup problems\n",
    "- Explain error messages\n",
    "- Guide users through configuration steps\n",
    "\n",
    "Guidelines:\n",
    "- Be patient and clear in your explanations\n",
    "- Ask diagnostic questions to identify the problem\n",
    "- Provide step-by-step solutions\n",
    "- If the issue requires developer attention, offer to create a support ticket\n",
    "\"\"\"\n",
    "    }\n",
    "]\n",
    "\n",
    "def tech_support_chat(user_message):\n",
    "    \"\"\"Tech support chat function.\"\"\"\n",
    "    tech_support_bot.append({\"role\": \"user\", \"content\": user_message})\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=tech_support_bot,\n",
    "        temperature=0.7,\n",
    "        max_tokens=200\n",
    "    )\n",
    "    \n",
    "    assistant_message = response.choices[0].message.content\n",
    "    tech_support_bot.append({\"role\": \"assistant\", \"content\": assistant_message})\n",
    "    \n",
    "    return assistant_message\n",
    "\n",
    "# Simulate tech support interactions\n",
    "test_messages = [\n",
    "    \"My application keeps crashing when I try to export a file.\",\n",
    "    \"I'm using version 2.5 on Windows 11.\",\n",
    "    \"I tried that but it's still not working. What else can I do?\"\n",
    "]\n",
    "\n",
    "for msg in test_messages:\n",
    "    print(f\"üë§ User: {msg}\")\n",
    "    response = tech_support_chat(msg)\n",
    "    print(f\"ü§ñ Bot: {response}\\n\")\n",
    "    print(\"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Key Takeaways\n",
    "\n",
    "1. **LLMs predict the next word** - that's the fundamental operation, but it enables complex behaviors\n",
    "\n",
    "2. **Conversation requires state** - you must include message history in each API call\n",
    "\n",
    "3. **System prompts are powerful** - they shape behavior without the user seeing them\n",
    "\n",
    "4. **Structure matters** - messages have roles (system, user, assistant) that guide the model\n",
    "\n",
    "## üí∞ Cost Considerations\n",
    "\n",
    "Each API call costs money based on tokens:\n",
    "- System prompt: counted every time\n",
    "- Conversation history: grows with each turn\n",
    "- New message: adds more tokens\n",
    "- Response: output tokens cost more!\n",
    "\n",
    "**Pro tip**: Long conversations get expensive. Consider truncating old history in production.\n",
    "\n",
    "## üöÄ Next Steps\n",
    "\n",
    "Try these experiments:\n",
    "1. Change the temperature (0 = deterministic, 1+ = creative)\n",
    "2. Modify the system prompt to create different personalities\n",
    "3. Add intent classification before generating responses\n",
    "4. Implement conversation summarization for long chats"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
