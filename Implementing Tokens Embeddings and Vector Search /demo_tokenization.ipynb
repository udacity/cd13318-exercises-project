{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Demo: Tokenization and Cost Analysis\n",
        "\n",
        "In this demo, you'll learn how LLMs break text into tokens and why it matters for your wallet! We'll explore:\n",
        "\n",
        "1. **What are tokens?** - How LLMs see text\n",
        "2. **Token counting** - Using tiktoken to count tokens\n",
        "3. **Cost calculations** - Understanding API pricing\n",
        "4. **Conversation growth** - How chat history affects costs\n",
        "5. **Optimization strategies** - Reducing costs without sacrificing quality\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "By the end of this demo, you'll understand:\n",
        "- How text gets converted into tokens\n",
        "- Why token count matters more than word count\n",
        "- How to calculate API costs for your application\n",
        "- Strategies to optimize token usage and reduce costs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n",
        "\n",
        "First, let's install and import the required libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install tiktoken if needed\n",
        "# !pip install tiktoken matplotlib pandas\n",
        "\n",
        "import tiktoken\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from openai import OpenAI\n",
        "import os\n",
        "\n",
        "# Initialize OpenAI client\n",
        "api_key = os.getenv(\"OPENAI_API_KEY\", \"your-api-key-here\")\n",
        "client = OpenAI(\n",
        "    api_key=api_key,\n",
        "    base_url=\"https://openai.vocareum.com/v1\" if api_key.startswith(\"voc\") else None\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 1: What Are Tokens?\n",
        "\n",
        "Tokens are **chunks of text** that the LLM processes. They're not quite words!\n",
        "\n",
        "- Common words = 1 token (\"hello\", \"world\")\n",
        "- Uncommon words = multiple tokens (\"tokenization\" might be 2-3)\n",
        "- Numbers and symbols = often 1 token each\n",
        "- Spaces and punctuation = separate tokens\n",
        "\n",
        "Let's see how different texts tokenize!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get the tokenizer for GPT-3.5/GPT-4\n",
        "encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
        "\n",
        "def show_tokens(text):\n",
        "    \"\"\"Show how text is split into tokens.\"\"\"\n",
        "    tokens = encoding.encode(text)\n",
        "    token_strings = [encoding.decode([token]) for token in tokens]\n",
        "    \n",
        "    print(f\"Text: '{text}'\")\n",
        "    print(f\"Token count: {len(tokens)}\")\n",
        "    print(f\"Tokens: {token_strings}\\n\")\n",
        "    \n",
        "    return len(tokens)\n",
        "\n",
        "# Examples\n",
        "print(\"üìù Simple Examples:\\n\")\n",
        "show_tokens(\"Hello world\")\n",
        "show_tokens(\"Hello, world!\")\n",
        "show_tokens(\"Tokenization\")\n",
        "show_tokens(\"AI is amazing\")\n",
        "show_tokens(\"GPT-3.5-turbo\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üí° Key Insight\n",
        "\n",
        "Notice how:\n",
        "- Punctuation adds tokens!\n",
        "- Technical terms may split into multiple tokens\n",
        "- Numbers and special characters count as tokens\n",
        "\n",
        "**Rule of thumb**: 1 token ‚âà 4 characters for English text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 2: Token Count Comparisons\n",
        "\n",
        "Let's compare how different types of messages tokenize."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sample customer service messages\n",
        "messages = {\n",
        "    \"Short query\": \"Where is my order?\",\n",
        "    \"Medium query\": \"I ordered a product 5 days ago but haven't received a tracking number. Can you help?\",\n",
        "    \"Long query\": \"I placed an order on January 15th for three items: a blue backpack, a water bottle, and hiking boots. I received an email confirmation but no shipping notification. My order number is #12345. Can you check the status and let me know when it will arrive? I need it by next week for a trip.\",\n",
        "    \"Technical\": \"ERROR: NullPointerException in payment.processTransaction() at line 247\",\n",
        "    \"With emojis\": \"Thanks so much! üòä You've been really helpful! üéâ Have a great day! ‚≠ê\"\n",
        "}\n",
        "\n",
        "# Count tokens for each\n",
        "results = []\n",
        "for msg_type, text in messages.items():\n",
        "    token_count = len(encoding.encode(text))\n",
        "    word_count = len(text.split())\n",
        "    char_count = len(text)\n",
        "    \n",
        "    results.append({\n",
        "        \"Message Type\": msg_type,\n",
        "        \"Characters\": char_count,\n",
        "        \"Words\": word_count,\n",
        "        \"Tokens\": token_count,\n",
        "        \"Tokens/Word\": f\"{token_count/word_count:.2f}\" if word_count > 0 else \"N/A\"\n",
        "    })\n",
        "\n",
        "# Display as a table\n",
        "df = pd.DataFrame(results)\n",
        "print(\"üìä Token Count Comparison:\\n\")\n",
        "print(df.to_string(index=False))\n",
        "\n",
        "# Show actual messages\n",
        "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
        "print(\"üì® Sample Messages:\\n\")\n",
        "for msg_type, text in messages.items():\n",
        "    print(f\"{msg_type}: \\\"{text}\\\"\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 3: API Cost Calculations\n",
        "\n",
        "Now let's calculate the actual costs! OpenAI pricing (as of 2024):\n",
        "\n",
        "| Model | Input (per 1K tokens) | Output (per 1K tokens) |\n",
        "|-------|----------------------|------------------------|\n",
        "| GPT-3.5-turbo | $0.0005 | $0.0015 |\n",
        "| GPT-4 | $0.03 | $0.06 |\n",
        "| GPT-4-turbo | $0.01 | $0.03 |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pricing (per 1000 tokens)\n",
        "PRICING = {\n",
        "    \"gpt-3.5-turbo\": {\"input\": 0.0005, \"output\": 0.0015},\n",
        "    \"gpt-4\": {\"input\": 0.03, \"output\": 0.06},\n",
        "    \"gpt-4-turbo\": {\"input\": 0.01, \"output\": 0.03}\n",
        "}\n",
        "\n",
        "def calculate_cost(input_tokens, output_tokens, model=\"gpt-3.5-turbo\"):\n",
        "    \"\"\"Calculate the cost of an API call.\"\"\"\n",
        "    input_cost = (input_tokens / 1000) * PRICING[model][\"input\"]\n",
        "    output_cost = (output_tokens / 1000) * PRICING[model][\"output\"]\n",
        "    total_cost = input_cost + output_cost\n",
        "    \n",
        "    return {\n",
        "        \"input_cost\": input_cost,\n",
        "        \"output_cost\": output_cost,\n",
        "        \"total_cost\": total_cost\n",
        "    }\n",
        "\n",
        "# Example: Customer service interaction\n",
        "system_prompt = \"\"\"You are a helpful customer service assistant for ShopEasy. \n",
        "Be professional, empathetic, and provide clear information.\"\"\"\n",
        "\n",
        "user_message = \"I ordered a product 5 days ago but haven't received a tracking number. Can you help?\"\n",
        "\n",
        "# Typical response length\n",
        "assistant_response = \"\"\"I apologize for the delay in receiving your tracking information. \n",
        "I'd be happy to help you track your order. Could you please provide me with your order number? \n",
        "It should be in your confirmation email and starts with #. Once I have that, \n",
        "I can look up the current status and provide you with tracking details.\"\"\"\n",
        "\n",
        "# Count tokens\n",
        "system_tokens = len(encoding.encode(system_prompt))\n",
        "user_tokens = len(encoding.encode(user_message))\n",
        "response_tokens = len(encoding.encode(assistant_response))\n",
        "\n",
        "input_tokens = system_tokens + user_tokens\n",
        "output_tokens = response_tokens\n",
        "\n",
        "print(\"üéØ Single Interaction Breakdown:\\n\")\n",
        "print(f\"System prompt: {system_tokens} tokens\")\n",
        "print(f\"User message: {user_tokens} tokens\")\n",
        "print(f\"Assistant response: {response_tokens} tokens\")\n",
        "print(f\"\\nTotal input: {input_tokens} tokens\")\n",
        "print(f\"Total output: {output_tokens} tokens\\n\")\n",
        "\n",
        "# Calculate costs for different models\n",
        "print(\"üí∞ Cost Comparison by Model:\\n\")\n",
        "for model in PRICING.keys():\n",
        "    costs = calculate_cost(input_tokens, output_tokens, model)\n",
        "    print(f\"{model}:\")\n",
        "    print(f\"  Input cost:  ${costs['input_cost']:.6f}\")\n",
        "    print(f\"  Output cost: ${costs['output_cost']:.6f}\")\n",
        "    print(f\"  Total cost:  ${costs['total_cost']:.6f}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üí° Key Insight\n",
        "\n",
        "Notice that:\n",
        "- **Output tokens cost MORE** than input tokens (often 2-3x)\n",
        "- **GPT-4 is 60x more expensive** than GPT-3.5-turbo\n",
        "- A single interaction costs fractions of a cent\n",
        "- But thousands of interactions add up quickly!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 4: Conversation History Token Growth\n",
        "\n",
        "The real cost killer is **conversation history**. Each message adds to the context sent with every request!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simulate a conversation\n",
        "conversation = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful customer service assistant for ShopEasy.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Where is my order?\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"I'd be happy to help! Could you provide your order number?\"},\n",
        "    {\"role\": \"user\", \"content\": \"Order #12345\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"Thank you! Let me check that for you. Your order shipped yesterday and should arrive in 2-3 business days. Your tracking number is TRK789456.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Can I change the delivery address?\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"I understand you'd like to change the delivery address. Unfortunately, since the order has already shipped, we can't modify the address directly. However, you can contact the carrier using your tracking number to request a delivery redirect.\"},\n",
        "    {\"role\": \"user\", \"content\": \"How do I contact the carrier?\"},\n",
        "]\n",
        "\n",
        "# Calculate cumulative tokens at each turn\n",
        "cumulative_tokens = []\n",
        "turn_numbers = []\n",
        "turn_costs = []\n",
        "\n",
        "for i in range(1, len(conversation) + 1):\n",
        "    # Count tokens in conversation up to this point\n",
        "    messages_so_far = conversation[:i]\n",
        "    total_tokens = sum(len(encoding.encode(msg[\"content\"])) for msg in messages_so_far)\n",
        "    \n",
        "    cumulative_tokens.append(total_tokens)\n",
        "    turn_numbers.append(i)\n",
        "    \n",
        "    # Estimate cost (assuming average 50 token response)\n",
        "    cost = calculate_cost(total_tokens, 50, \"gpt-3.5-turbo\")[\"total_cost\"]\n",
        "    turn_costs.append(cost)\n",
        "\n",
        "# Show the data\n",
        "print(\"üìà Token Growth Over Conversation:\\n\")\n",
        "for i, (turn, tokens, cost) in enumerate(zip(turn_numbers, cumulative_tokens, turn_costs)):\n",
        "    role = conversation[i][\"role\"]\n",
        "    print(f\"Turn {turn} ({role}): {tokens} tokens (${cost:.6f} per API call)\")\n",
        "\n",
        "# Visualize\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(turn_numbers, cumulative_tokens, marker='o', linewidth=2, markersize=8)\n",
        "plt.xlabel('Conversation Turn')\n",
        "plt.ylabel('Cumulative Tokens')\n",
        "plt.title('Token Count Growth')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(turn_numbers, turn_costs, marker='s', color='orange', linewidth=2, markersize=8)\n",
        "plt.xlabel('Conversation Turn')\n",
        "plt.ylabel('Cost per API Call ($)')\n",
        "plt.title('Cost Growth (GPT-3.5-turbo)')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\n‚ö†Ô∏è Final conversation: {cumulative_tokens[-1]} tokens\")\n",
        "print(f\"That's {cumulative_tokens[-1] / cumulative_tokens[0]:.1f}x the initial size!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üö® The Problem\n",
        "\n",
        "As conversations grow:\n",
        "- Each API call includes ALL previous messages\n",
        "- Token count grows linearly (or worse)\n",
        "- Costs increase with each turn\n",
        "- You hit context limits (4K, 8K, 16K tokens depending on model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 5: Message Chunking Demonstration\n",
        "\n",
        "For long texts, you might need to chunk them. Let's see how!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def chunk_text_by_tokens(text, max_tokens=100, overlap=20):\n",
        "    \"\"\"Split text into chunks with a maximum token count.\"\"\"\n",
        "    tokens = encoding.encode(text)\n",
        "    chunks = []\n",
        "    \n",
        "    start = 0\n",
        "    while start < len(tokens):\n",
        "        # Get chunk\n",
        "        end = start + max_tokens\n",
        "        chunk_tokens = tokens[start:end]\n",
        "        chunk_text = encoding.decode(chunk_tokens)\n",
        "        \n",
        "        chunks.append({\n",
        "            \"text\": chunk_text,\n",
        "            \"tokens\": len(chunk_tokens),\n",
        "            \"start\": start,\n",
        "            \"end\": end\n",
        "        })\n",
        "        \n",
        "        # Move start forward, accounting for overlap\n",
        "        start = end - overlap\n",
        "    \n",
        "    return chunks\n",
        "\n",
        "# Long customer message\n",
        "long_message = \"\"\"I'm writing to express my frustration with my recent order experience. \n",
        "I ordered three items on January 15th: a blue hiking backpack (SKU: BP-123), a stainless \n",
        "steel water bottle (SKU: WB-456), and a pair of hiking boots size 10 (SKU: HB-789). \n",
        "My order number is #12345. I received a confirmation email immediately, which was great, \n",
        "but then nothing for five days. No shipping notification, no tracking number, nothing. \n",
        "I tried to check the order status on your website, but the tracking page just shows \n",
        "'processing' with no additional information. I called customer service twice, and both \n",
        "times I was on hold for over 30 minutes before giving up. I really need these items by \n",
        "next Friday because I'm leaving for a week-long hiking trip, and I specifically chose \n",
        "expedited shipping at checkout, which cost me an extra $25. Can someone please help me \n",
        "figure out where my order is and when it will actually arrive? This is very time-sensitive.\"\"\"\n",
        "\n",
        "print(f\"Original text: {len(encoding.encode(long_message))} tokens\\n\")\n",
        "print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "# Chunk it\n",
        "chunks = chunk_text_by_tokens(long_message, max_tokens=100, overlap=20)\n",
        "\n",
        "print(f\"üì¶ Split into {len(chunks)} chunks:\\n\")\n",
        "for i, chunk in enumerate(chunks, 1):\n",
        "    print(f\"Chunk {i} ({chunk['tokens']} tokens):\")\n",
        "    print(f\"  \\\"{chunk['text'][:100]}...\\\"\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 6: Cost Optimization Strategies\n",
        "\n",
        "Let's explore techniques to reduce costs without sacrificing quality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Strategy comparison\n",
        "strategies = {\n",
        "    \"Baseline (keep all history)\": {\n",
        "        \"description\": \"Include entire conversation in every request\",\n",
        "        \"tokens_per_turn\": [50, 120, 200, 290, 380, 480, 590, 710],\n",
        "        \"quality\": \"High\",\n",
        "        \"context_retention\": \"100%\"\n",
        "    },\n",
        "    \"Keep last 3 messages\": {\n",
        "        \"description\": \"Only include system + last 3 messages\",\n",
        "        \"tokens_per_turn\": [50, 120, 200, 200, 200, 200, 200, 200],\n",
        "        \"quality\": \"Medium-High\",\n",
        "        \"context_retention\": \"~40%\"\n",
        "    },\n",
        "    \"Summarize old messages\": {\n",
        "        \"description\": \"Summarize messages older than 4 turns\",\n",
        "        \"tokens_per_turn\": [50, 120, 200, 290, 320, 350, 380, 410],\n",
        "        \"quality\": \"High\",\n",
        "        \"context_retention\": \"~70%\"\n",
        "    },\n",
        "    \"No history (stateless)\": {\n",
        "        \"description\": \"Only send current message\",\n",
        "        \"tokens_per_turn\": [50, 50, 50, 50, 50, 50, 50, 50],\n",
        "        \"quality\": \"Low\",\n",
        "        \"context_retention\": \"0%\"\n",
        "    }\n",
        "}\n",
        "\n",
        "# Calculate costs for each strategy\n",
        "print(\"üí° Cost Optimization Strategies:\\n\")\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "for strategy_name, data in strategies.items():\n",
        "    tokens = data[\"tokens_per_turn\"]\n",
        "    # Calculate cumulative cost (sum of all API calls)\n",
        "    cumulative_cost = sum(\n",
        "        calculate_cost(t, 50, \"gpt-3.5-turbo\")[\"total_cost\"] \n",
        "        for t in tokens\n",
        "    )\n",
        "    \n",
        "    print(f\"üìä {strategy_name}:\")\n",
        "    print(f\"   {data['description']}\")\n",
        "    print(f\"   Total cost (8 turns): ${cumulative_cost:.6f}\")\n",
        "    print(f\"   Quality: {data['quality']}\")\n",
        "    print(f\"   Context retention: {data['context_retention']}\")\n",
        "    print()\n",
        "    \n",
        "    # Plot\n",
        "    plt.plot(range(1, len(tokens) + 1), tokens, marker='o', label=strategy_name, linewidth=2)\n",
        "\n",
        "plt.xlabel('Conversation Turn')\n",
        "plt.ylabel('Input Tokens')\n",
        "plt.title('Token Usage: Strategy Comparison')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nüéØ Recommendation: Use 'Summarize old messages' for best balance of cost and quality!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 7: Real-World Cost Projection\n",
        "\n",
        "Let's calculate costs for a realistic customer service application."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Assumptions for a customer service chatbot\n",
        "assumptions = {\n",
        "    \"daily_users\": 1000,\n",
        "    \"avg_messages_per_user\": 5,\n",
        "    \"avg_input_tokens_per_message\": 200,  # includes history\n",
        "    \"avg_output_tokens_per_message\": 100,\n",
        "    \"model\": \"gpt-3.5-turbo\"\n",
        "}\n",
        "\n",
        "# Calculate costs\n",
        "messages_per_day = assumptions[\"daily_users\"] * assumptions[\"avg_messages_per_user\"]\n",
        "cost_per_message = calculate_cost(\n",
        "    assumptions[\"avg_input_tokens_per_message\"],\n",
        "    assumptions[\"avg_output_tokens_per_message\"],\n",
        "    assumptions[\"model\"]\n",
        ")[\"total_cost\"]\n",
        "\n",
        "daily_cost = messages_per_day * cost_per_message\n",
        "monthly_cost = daily_cost * 30\n",
        "yearly_cost = daily_cost * 365\n",
        "\n",
        "print(\"üí∞ Cost Projection for Customer Service Bot:\\n\")\n",
        "print(\"Assumptions:\")\n",
        "for key, value in assumptions.items():\n",
        "    print(f\"  {key.replace('_', ' ').title()}: {value}\")\n",
        "\n",
        "print(f\"\\nCalculated Metrics:\")\n",
        "print(f\"  Messages per day: {messages_per_day:,}\")\n",
        "print(f\"  Cost per message: ${cost_per_message:.6f}\")\n",
        "\n",
        "print(f\"\\nüìä Projected Costs:\")\n",
        "print(f\"  Daily:   ${daily_cost:.2f}\")\n",
        "print(f\"  Monthly: ${monthly_cost:.2f}\")\n",
        "print(f\"  Yearly:  ${yearly_cost:,.2f}\")\n",
        "\n",
        "# Compare with GPT-4\n",
        "gpt4_cost_per_message = calculate_cost(\n",
        "    assumptions[\"avg_input_tokens_per_message\"],\n",
        "    assumptions[\"avg_output_tokens_per_message\"],\n",
        "    \"gpt-4\"\n",
        ")[\"total_cost\"]\n",
        "gpt4_yearly = messages_per_day * gpt4_cost_per_message * 365\n",
        "\n",
        "print(f\"\\nüîÑ If using GPT-4 instead:\")\n",
        "print(f\"  Yearly cost: ${gpt4_yearly:,.2f}\")\n",
        "print(f\"  Difference: ${gpt4_yearly - yearly_cost:,.2f} more per year\")\n",
        "print(f\"  That's {gpt4_yearly / yearly_cost:.1f}x more expensive!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ Key Takeaways\n",
        "\n",
        "1. **Tokens ‚â† Words** - Tokenization is model-specific and affects costs directly\n",
        "\n",
        "2. **Output costs more** - Generated tokens are 2-3x more expensive than input\n",
        "\n",
        "3. **Conversation history grows** - Each turn adds tokens, costs increase linearly\n",
        "\n",
        "4. **Model choice matters** - GPT-4 is 60x more expensive than GPT-3.5-turbo\n",
        "\n",
        "5. **Optimize strategically** - Summarize old messages or limit history to save costs\n",
        "\n",
        "6. **Count before you commit** - Use tiktoken to estimate costs before deployment\n",
        "\n",
        "## üí° Cost Optimization Tips\n",
        "\n",
        "1. **Truncate history**: Keep only the last N messages\n",
        "2. **Summarize context**: Compress old messages into summaries\n",
        "3. **Use cheaper models**: GPT-3.5-turbo for simple tasks, GPT-4 for complex ones\n",
        "4. **Limit output**: Set max_tokens to avoid unnecessarily long responses\n",
        "5. **Cache results**: Don't regenerate answers to common questions\n",
        "6. **Smart routing**: Use embeddings/classification to route to appropriate model\n",
        "\n",
        "## üöÄ Try It Yourself!\n",
        "\n",
        "Experiments to run:\n",
        "1. Calculate costs for your specific use case\n",
        "2. Compare different message truncation strategies\n",
        "3. Measure token counts for your actual prompts\n",
        "4. Test chunking strategies for long documents"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
